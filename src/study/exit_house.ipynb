{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class House(object):\n",
    "    def __init__(self):\n",
    "        self.state = [0, 1, 2, 3, 4, 5]\n",
    "        self.goal_state = [5]\n",
    "        self.action = [\n",
    "            [4],\n",
    "            [3, 5],\n",
    "            [3],\n",
    "            [1, 2, 4],\n",
    "            [0, 3, 5],\n",
    "            [1, 4, 5]\n",
    "        ]\n",
    "        \n",
    "        self.reward = {0: {4: 0},\n",
    "                        1: {3: 0,\n",
    "                            5: 100},\n",
    "                        2: {3: 0},\n",
    "                        3: {1: 0,\n",
    "                            2: 0,\n",
    "                            4: 0},\n",
    "                        4: {0: 0,\n",
    "                            3: 0,\n",
    "                            5: 100},\n",
    "                        5: {1: 0,\n",
    "                            4: 0,\n",
    "                            5: 100}\n",
    "                        }\n",
    "\n",
    "    def num_state(self):\n",
    "        return len(self.state)\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return self.action[state]\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        if not self.reward.has_key(state):\n",
    "            return -1\n",
    "\n",
    "        if not self.reward[state].has_key(action):\n",
    "            return -1\n",
    "\n",
    "        return self.reward[state][action]\n",
    "\n",
    "    def get_init_state(self):\n",
    "        return random.randint(0, self.num_state() - 1)\n",
    "\n",
    "    def is_goal(self, state):\n",
    "        if state in self.goal_state:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, env, q_function):\n",
    "        self.state = 0\n",
    "        self.trajectory = []\n",
    "        self.env = env\n",
    "        self.q_function = q_function\n",
    "\n",
    "    def begin(self):\n",
    "        init_state = self.env.get_init_state()\n",
    "        self.state = init_state\n",
    "        self.trajectory = []\n",
    "\n",
    "    def do_epsilon_greedy(self):\n",
    "        e = 1.0  # random\n",
    "        pivot = np.random.uniform()\n",
    "        if pivot < e:\n",
    "            action_list = self.env.get_actions(self.state)\n",
    "            action = random.choice(action_list)\n",
    "        else:\n",
    "            action = self.q_function.get_max_action(self.state)\n",
    "        reward = self.env.get_reward(self.state, action)\n",
    "        self.trajectory.append((self.state, action, reward))\n",
    "        self.state = action\n",
    "\n",
    "    def finalize_trajectory(self):\n",
    "        self.trajectory.append((self.state, -1, -1))\n",
    "\n",
    "    def is_goal(self):\n",
    "        return self.env.is_goal(self.state)\n",
    "\n",
    "    def reward(self):\n",
    "        return sum([trj[2] for trj in self.trajectory])\n",
    "\n",
    "    def make_experience(self):\n",
    "        self.begin()\n",
    "        while not self.is_goal():\n",
    "            self.do_epsilon_greedy()\n",
    "        self.finalize_trajectory()\n",
    "\n",
    "\n",
    "class QFunction(object):\n",
    "    def __init__(self, env):\n",
    "        self.q_tbl = np.zeros([env.num_state(), env.num_state()])\n",
    "        self.env = env\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        return self.q_tbl[state][action]\n",
    "\n",
    "    def get_max_value(self, state):\n",
    "        return max(self.q_tbl[state])\n",
    "\n",
    "    def get_max_action(self, state):\n",
    "        act_list = self.env.get_actions(state)\n",
    "        q_list = [self.q_tbl[state][act] for act in act_list]\n",
    "        max_act_id = np.argmax(q_list)\n",
    "        return act_list[max_act_id]\n",
    "\n",
    "    def update_value(self, state, action, value):\n",
    "        self.q_tbl[state][action] = value\n",
    "\n",
    "    def update(self, trajectory, gamma=0.8):\n",
    "        q_tbl_before = self.q_tbl.copy()\n",
    "        len_traj = len(trajectory)\n",
    "\n",
    "        updated = False\n",
    "        j = 0\n",
    "        while j < len_traj - 1:\n",
    "            state, action, reward = trajectory[j]\n",
    "\n",
    "            next_state = trajectory[j + 1][0]\n",
    "\n",
    "            max_q = max(q_tbl_before[next_state])\n",
    "            new_q = reward + gamma * max_q\n",
    "\n",
    "            if new_q != q_tbl_before[state][action]:\n",
    "                updated = True\n",
    "\n",
    "            self.update_value(state, action, new_q)\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        return updated\n",
    "\n",
    "    def print_learned(self):\n",
    "        max_val = self.q_tbl.max()\n",
    "        val = (self.q_tbl / max_val * 100).astype(dtype=np.uint)\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0: [(0, 4, 0), (4, 3, 0), (3, 4, 0), (4, 0, 0), (0, 4, 0), (4, 3, 0), (3, 1, 0), (1, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 1: [(2, 3, 0), (3, 4, 0), (4, 0, 0), (0, 4, 0), (4, 0, 0), (0, 4, 0), (4, 3, 0), (3, 1, 0), (1, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 100]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0  80   0   0   0   0]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 2: [(0, 4, 0), (4, 3, 0), (3, 1, 0), (1, 3, 0), (3, 1, 0), (1, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0  80   0   0   0   0]\n",
      " [  0   0   0  64   0   0]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 3: [(3, 4, 0), (4, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]\n",
      " [  0  80   0   0  51   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 4: [(3, 2, 0), (2, 3, 0), (3, 1, 0), (1, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0   0   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0  64   0   0]\n",
      " [  0  80   0   0  51   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 5: [(0, 4, 0), (4, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0  80   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0  64   0   0]\n",
      " [  0  80   0   0  51   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 7: [(3, 4, 0), (4, 0, 0), (0, 4, 0), (4, 0, 0), (0, 4, 0), (4, 3, 0), (3, 1, 0), (1, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0  80   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0  64   0   0]\n",
      " [  0  80   0   0  80   0]\n",
      " [ 64   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]]\n",
      "episode 8: [(3, 2, 0), (2, 3, 0), (3, 2, 0), (2, 3, 0), (3, 2, 0), (2, 3, 0), (3, 4, 0), (4, 3, 0), (3, 1, 0), (1, 3, 0), (3, 4, 0), (4, 0, 0), (0, 4, 0), (4, 3, 0), (3, 1, 0), (1, 5, 100), (5, -1, -1)]\n",
      "[[  0   0   0   0  80   0]\n",
      " [  0   0   0  64   0 100]\n",
      " [  0   0   0  64   0   0]\n",
      " [  0  80  51   0  80   0]\n",
      " [ 64   0   0  64   0 100]\n",
      " [  0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "def q_learning():\n",
    "    env = House()\n",
    "    q_function = QFunction(env)\n",
    "    agent = Agent(env, q_function)\n",
    "\n",
    "    no_update_count = 0\n",
    "    max_episode = 50\n",
    "    i = 0\n",
    "    while i < max_episode:\n",
    "        agent.make_experience()\n",
    "\n",
    "        updated = q_function.update(agent.trajectory)\n",
    "\n",
    "        if updated == False:\n",
    "            no_update_count += 1\n",
    "        else:\n",
    "            print('episode %d: %s' % (i, agent.trajectory))\n",
    "            q_function.print_learned()\n",
    "\n",
    "        if no_update_count == 10:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
